\begin{tcolorbox}[title=Problem 1, breakable]
    Show that in a ring, $0a = a0 = 0$.
\end{tcolorbox}

\begin{proof}
    Let $R$ be a ring and $a$ be an arbitrary element in $R$.
    Then
    \begin{align*}
        0a &= 0a + 0 &&\text{Rule 3} \\
            &= 0a + (0a + (-0a)) &&\text{Rule 4} \\
            &= (0a + 0a) + (-0a) &&\text{Rule 2} \\
            &= ((0 + 0)a) + (-0a) &&\text{Rule 6} \\
            &= 0a + (-0a) &&\text{Rule 3} \\
            &= 0 &&\text{Rule 3}
    \end{align*}
    Similarly 
    \begin{align*}
        a0 &= a0 + 0 &&\text{Rule 3} \\
            &= a0 + (a0 + (-a0)) &&\text{Rule 4} \\
            &= (a0 + a0) + (-a0) &&\text{Rule 2} \\
            &= (a(0 + 0)) + (-a0) &&\text{Rule 6} \\
            &= a0 + (-a0) &&\text{Rule 3} \\
            &= 0 &&\text{Rule 3}
    \end{align*}
    Thus $a0 = 0a = 0$.
\end{proof}

\begin{tcolorbox}[title=Problem 2, breakable]
    Prove part d of Theorem 6.1:
    Show that in a ring the additive identity is unique,
        by supposing $0$ and $0'$ satisfy Rule 3
        and proving that $0 = 0'$.
\end{tcolorbox}

\begin{proof}
    Let $R$ be a ring and suppose there exists $0 \in R$
        and $0' \in R$ such that 
        for all $a \in R$, $a + 0 = a$ and $a + 0' = a$.
    Then $a + 0 = a = a + 0'$.
    By Additive Cancellation $0 = 0'$.
\end{proof}

\begin{tcolorbox}[title=Problem 3, breakable]
    Show that in a ring $(-a)b = a(-b) = -(ab)$.
\end{tcolorbox}

\begin{proof}
    Let $R$ be a ring and $a, b \in R$. Then
    \begin{align*}
        0 = (a + (-a))b 
        \iff &0 = ab + (-a)b &&\text{Rule 6} \\
        \iff &-(ab) + 0 = -(ab) + (ab + (-a)b) \\
        \iff &-(ab) = -(ab) + (ab + (-a)b) &&\text{Rule 3} \\
        \iff &-(ab) = (-(ab) + ab) + (-a)b &&\text{Rule 2} \\
        \iff &-(ab) = 0 + (-a)b &&\text{Rule 4} \\
        \iff &-(ab) = (-a)b + 0 &&\text{Rule 1} \\
        \iff &-(ab) = (-a)b &&\text{Rule 3}
    \end{align*}
    Also
    \begin{align*}
        0 = a(b + (-b)) 
        \iff &0 = ab + a(-b) &&\text{Rule 6} \\
        \iff &-(ab) + 0 = -(ab) + ((ab) + a(-b))  \\
        \iff &-(ab) = -(ab) + ((ab) + a(-b)) &&\text{Rule 3} \\
        \iff &-(ab) = (-(ab) + (ab)) + a(-b) &&\text{Rule 2} \\
        \iff &-(ab) = 0 + a(-b) &&\text{Rule 4} \\
        \iff &-(ab) = a(-b) + 0 &&\text{Rule 1} \\
        \iff &-(ab) = a(-b) &&\text{Rule 3}
    \end{align*}
    Thus $(-a)b = a(-b) = -(ab)$.
\end{proof}

\begin{tcolorbox}[title=Problem 4, breakable]
    Show that in a ring $(-a)(-b) = ab$.
\end{tcolorbox}

\begin{proof}
    \begin{align*}
        (-a)(-b) + -(ab) &= a(-(-b)) + a(-b) && \text{Rule 6} \\
                        &= a((-(-b)) + (-b)) && \text{Rule 2} \\
                        &= a \cdot 0 && \text{Rule 4} \\
                        &= 0 && \text{Rule 3}
    \end{align*}
    Thus $(-a)(-b) = ab$.
\end{proof}

\begin{tcolorbox}[title=Problem 5, breakable]
    Prove the following facts about subtraction in a ring $R$,
    where $a, b, c \in R$.

    (a) $a - a = 0$.

    (b) $a(b - c) = ab - ac$.

    (c) $(b - c)a = ba - ca$.
\end{tcolorbox}

\begin{proof}
    Let $R$ be a ring and $a, b, c \in R$.
    Then $a - a = a + (-a) = 0$ Rule 4.
    Also, $a(b - c) = a(b + (-c)) = ab + a(-c)$ Rule 6.
    Then, $ab + a(-c) = ab + -(ac) \text{ Problem 3 } = ab - ac$.
    Similarly, $(b - c)a = (b + (-c))a = ba + (-c)a$ Rule 6.
    Then, $ba + (-c)a = ba + -(ca) \text{ Problem 3 } = ba - ca$.
\end{proof}

\begin{tcolorbox}[title=Problem 8, breakable]
    We generalize Exercises 6 and 7: Let $R$
    be any commutative ring (other than the zero ring).
    Define $M_2(R)$ as the set of $2 \times 2$
    matrices with entries from $R$. Show that $M_2(R)$
    is a ring which is not commutative. (Note that 
    for the most part the proofs in Exercises 6 and 
    7 life over without change.)
\end{tcolorbox}

\begin{proof}
    Let $R$ be a ring and 
    \[a, b, c, e, f, g, h, i, j, k, l \in R\]
    We first show associativity with respect to $+$.
    \[\begin{bmatrix}
        a & b \\
        c & d
    \end{bmatrix} \in M_2(R) \text{, }
    \begin{bmatrix}
        c & f \\
        g & h
    \end{bmatrix} \in M_2(R) \text{ and }
    \begin{bmatrix}
        i & j \\
        k & l
    \end{bmatrix} \in M_2(R)\]
    Then 
    \[\begin{bmatrix}
        a & b \\
        c & d
    \end{bmatrix} + \begin{bmatrix}
        c & f \\
        g & h
    \end{bmatrix} = \begin{bmatrix}
        a+c & b+f \\
        c+g & d+h
    \end{bmatrix} = \begin{bmatrix}
        c+a & f+b \\
        g+c & h+d
    \end{bmatrix} = \begin{bmatrix}
        c & f \\
        g & h
    \end{bmatrix} + \begin{bmatrix}
        a & b \\
        c & d
    \end{bmatrix}\]
    We now show associativity with respect to $+$.
\[\left(\begin{bmatrix}
        a & b \\
        c & d
    \end{bmatrix} + \begin{bmatrix}
        c & f \\
        g & h
    \end{bmatrix}\right) + \begin{bmatrix}
        i & j \\
        k & l
    \end{bmatrix} = \begin{bmatrix}
        a+c & b+f \\
        c+g & d+h
    \end{bmatrix} +\begin{bmatrix}
        i & j \\
        k & l
    \end{bmatrix} = \begin{bmatrix}
        (a+c)+i & (b+f)+j \\
        (c+g)+k & (d+h)+l
    \end{bmatrix}\] \[= \begin{bmatrix}
        a+(c+i) & b+(f+j) \\
        c+(g+k) & d+(h+l)
    \end{bmatrix} = \begin{bmatrix}
        a & b \\
        c & d
    \end{bmatrix} + \begin{bmatrix}
        c+i & f+j \\
        g+k & h+l
    \end{bmatrix} = \begin{bmatrix}
        a & b \\
        c & d
    \end{bmatrix} + \left(\begin{bmatrix}
        c & f \\
        g & h
    \end{bmatrix} + \begin{bmatrix}
        i & j \\
        k & l
    \end{bmatrix}\right)\]
    We now show the existence of an additive inverse.
    \[\begin{bmatrix}
        a & b \\
        c & d
    \end{bmatrix} -\begin{bmatrix}
        a & b \\
        c & d
    \end{bmatrix} = \begin{bmatrix}
        a-a & b-b \\
        c-c & d-d
    \end{bmatrix} = \begin{bmatrix}
        0 & 0 \\
        0 & 0
    \end{bmatrix}\]
    We now show the existence of the additive identity.
    \[\begin{bmatrix}
        a & b \\
        c & d
    \end{bmatrix} + \begin{bmatrix}
        0 & 0 \\
        0 & 0
    \end{bmatrix} = \begin{bmatrix}
        a+0 & b+0 \\
        c+0 & d+0
    \end{bmatrix} = \begin{bmatrix}
        a & b \\
        c & d
    \end{bmatrix}\]
    We now show left distributivity.
    \[
    \begin{bmatrix}
        a & b \\
        c & d
    \end{bmatrix}
    \left(
    \begin{bmatrix}
        e & f \\
        g & h
    \end{bmatrix}
    +
    \begin{bmatrix}
        i & j \\
        k & l
    \end{bmatrix}
    \right)
    =
    \begin{bmatrix}
        a & b \\
        c & d
    \end{bmatrix}
    \begin{bmatrix}
        e+i & f+j \\
        g+k & h+l
    \end{bmatrix}
    =
    \begin{bmatrix}
        a(e+i) + b(g+k) & a(f+j) + b(h+l) \\
        c(e+i) + d(g+k) & c(f+j) + d(h+l)
    \end{bmatrix}
    \]
    \[
    =
    \begin{bmatrix}
        ae + bg & af + bh \\
        ce + dg & cf + dh
    \end{bmatrix}
    +
    \begin{bmatrix}
        ai + bk & aj + bl \\
        ci + dk & cj + dl
    \end{bmatrix}
    =
    \begin{bmatrix}
        a & b \\
        c & d
    \end{bmatrix}
    \begin{bmatrix}
        e & f \\
        g & h
    \end{bmatrix}
    +
    \begin{bmatrix}
        a & b \\
        c & d
    \end{bmatrix}
    \begin{bmatrix}
        i & j \\
        k & l
    \end{bmatrix}.
    \]
    We now show right distributivity.
    \[
    \left(
    \begin{bmatrix}
        e & f \\
        g & h
    \end{bmatrix}
    +
    \begin{bmatrix}
        i & j \\
        k & l
    \end{bmatrix}
    \right)
    \begin{bmatrix}
        a & b \\
        c & d
    \end{bmatrix}
    =
    \begin{bmatrix}
        e+i & f+j \\
        g+k & h+l
    \end{bmatrix}
    \begin{bmatrix}
        a & b \\
        c & d
    \end{bmatrix}
    =
    \begin{bmatrix}
        (e+i)a + (f+j)c & (e+i)b + (f+j)d \\
        (g+k)a + (h+l)c & (g+k)b + (h+l)d
    \end{bmatrix}
    \]
    \[
    =
    \begin{bmatrix}
        ea + fc & eb + fd \\
        ga + hc & gb + hd
    \end{bmatrix}
    +
    \begin{bmatrix}
        ia + jc & ib + jd \\
        ka + lc & kb + ld
    \end{bmatrix}
    =
    \begin{bmatrix}
        e & f \\
        g & h
    \end{bmatrix}
    \begin{bmatrix}
        a & b \\
        c & d
    \end{bmatrix}
    +
    \begin{bmatrix}
        i & j \\
        k & l
    \end{bmatrix}
    \begin{bmatrix}
        a & b \\
        c & d
    \end{bmatrix}
    \]
    We now show matrix multiplication is not commutative 
        by giving a counterexample.
    Consider
    \[
    \begin{bmatrix}
    1 & 2 \\
    0 & 1
    \end{bmatrix}
    \begin{bmatrix}
    1 & 0 \\
    3 & 1
    \end{bmatrix}
    =
    \begin{bmatrix}
    1+6 & 2 \\
    3 & 1
    \end{bmatrix}
    =
    \begin{bmatrix}
    7 & 2 \\
    3 & 1
    \end{bmatrix}
    \]
    But multiplying the same matrices in the opposite order gives
    \[
    \begin{bmatrix}
    1 & 0 \\
    3 & 1
    \end{bmatrix}
    \begin{bmatrix}
    1 & 2 \\
    0 & 1
    \end{bmatrix}
    =
    \begin{bmatrix}
    1 & 2 \\
    3 & 7
    \end{bmatrix}.
    \]
    Since
    \[
    \begin{bmatrix}
    7 & 2 \\
    3 & 1
    \end{bmatrix}
    \neq
    \begin{bmatrix}
    1 & 2 \\
    3 & 7
    \end{bmatrix}
    \]
    matrix multiplication is not commutative.
\end{proof}

Let $R = \{0, a, \ldots\}$ such that $0 \ne a$.
\[
A = \begin{bmatrix} 0 & a \\ 0 & 0 \end{bmatrix}, \quad
B = \begin{bmatrix} 0 & 0 \\ a & 0 \end{bmatrix}
\]
\[
AB = \begin{bmatrix} a^2 & 0 \\ 0 & 0 \end{bmatrix} \neq 
BA = \begin{bmatrix} 0 & 0 \\ 0 & a^2 \end{bmatrix}
\]

\begin{tcolorbox}[title=Problem 9, breakable]
    Check that Example 6.14 is indeed a ring; that is,
    let $C[0, 1]$ be a set of functions defined from the closed 
    unit interval $[0, 1]$ to the real numbers that 
    are continuous. Define the sum and product of two 
    functions point-wise: $(f + g)(x) = f(x) + g(x)$
    and $(fg)(x) = f(x)g(x)$. Show that $C[0, 1]$ is a 
    commutative ring. (You may use theorems from calculus).
\end{tcolorbox}

\begin{proof}
    Let $x \in [0, 1]$ and $f, g, l : [0, 1] \longrightarrow \mathbb{R}$.
    Since $f(x), g(x), l(x) \in \mathbb{R}$ and $\mathbb{R}$ is a ring, standard ring operations (associativity, distributivity, etc.) hold.
    We first show commutativity over addition
    \[
    (f + g)(x) = f(x) + g(x) = g(x) + f(x) = (g + f)(x)
    \]
    Next, associativity over addition
    \[
    ((f + g) + l)(x) = (f + g)(x) + l(x) = f(x) + g(x) + l(x) = f(x) + (g + l)(x) = (f + (g + l))(x).
    \]
    Existence of additive inverses
    \[
    (f + (-f))(x) = f(x) + (-f(x)) = 0
    \]
    Additive identity
    \[
    (f + 0)(x) = f(x) + 0(x) = f(x)
    \]
    Commutativity of multiplication
    \[
    (fg)(x) = f(x) g(x) = g(x) f(x) = (gf)(x)
    \]
    Associativity of multiplication
    \[
    ((fg)l)(x) = (fg)(x) l(x) = f(x) g(x) l(x) = f(x) (gl)(x) = (f(gl))(x)
    \]
    Distributivity from the left
    \[
    f(g + l)(x) = f(x)(g + l)(x) = f(x)(g(x) + l(x)) = f(x)g(x) + f(x)l(x) = (fg + fl)(x)
    \]
    Distributivity from the right
    \[
    (f + g)l(x) = (f + g)(x) \, l(x) = (f(x) + g(x))l(x) = f(x)l(x) + g(x)l(x) = (fl + gl)(x)
    \]
\end{proof}

\begin{tcolorbox}[title=Problem 11, breakable]
    Let $\mathbb{C}$ be the complex numbers. That inverse
    \[\mathbb{C} = \{a + bi \mid a, b, \in \mathbb{R}\}\]
    where $i$ is the square root of $-1$ (that is, $i \cdot i = 1$).
    Here 
    \[(a + bi) + (c + di) = (a + c) + (bi + di)\]
    and 
    \[(a + bi)(c + di) = (ac - bd) + (ad + bc)i\]
    Show that $\mathbb{C}$ is a commutative ring.
\end{tcolorbox}

\begin{proof}
    Let $a+bi,\; c+di,\; e+fi \in \mathbb{C}$.
    Commutativity of addition
    \[
    (a+bi)+(c+di)
    = (a+c) + (b+d)i
    = (c+a) + (d+b)i
    = (c+di)+(a+bi)
    \]
    Associativity of addition
    \[
    ((a+bi)+(c+di))+(e+fi)
    = (a+c+e) + (b+d+f)i
    \]
    \[
    = (a+bi) + ((c+di)+(e+fi))
    \]
    Additive identity
    \[
    (a+bi) + 0 = (a+0) + (b+0)i = a+bi
    \]
    Additive inverses
    \[
    (a+bi) + (-a - bi)
    = (a-a) + (b-b)i
    = 0
    \]
    Commutativity of multiplication
    \[
    (a+bi)(c+di)
    = (ac - bd) + (ad + bc)i
    = (c+di)(a+bi)
    \]
    Associativity of multiplication
    \[
    ((a+bi)(c+di))(e+fi)
    = (a+bi)((c+di)(e+fi))
    \]
    Left distributivity
    \[
    (a+bi)((c+di)+(e+fi))
    = (a+bi)((c+e) + (d+f)i)
    \]
    \[
    = a(c+e) - b(d+f) + (a(d+f) + b(c+e))i
    \]
    \[
    = (ac - bd) + (ad + bc)i \;+\; (ae - bf) + (af + be)i
    \]
    \[
    = (a+bi)(c+di) + (a+bi)(e+fi)
    \]
    Right distributivity
    \[
    ((a+bi)+(c+di))(e+fi)
    = ((a+c) + (b+d)i)(e+fi)
    \]
    \[
    = (a+c)e - (b+d)f + ((a+c)f + (b+d)e)i
    \]
    \[
    = (ae - bf) + (af + be)i \;+\; (ce - df) + (cf + de)i
    \]
    \[
    = (a+bi)(e+fi) + (c+di)(e+fi)
    \]
\end{proof}

\begin{tcolorbox}[title=Problem 15, breakable]
    Verify that 6.10 is a ring.
    Namely, let $R$ and $S$ be arbitrary rings.
    Define addition and subtraction appropriately
        to make $R \times S$ a ring,
        where $R \times S$ is the set of ordered pairs
        with the first entry from $R$ and second entry 
        from $S$. 
    Now generalize this to the set $R_1 \times R_2 \times \cdots R_n$
    of $n$-tuples with entries from the rings $R_i$.
    This new ring is called the \textbf{direct product}
    of the rings $R_i$.
\end{tcolorbox}

\begin{definition}
    Let $R, S$ be arbitrary rings.
    Let $(a, b), (c, d) \in R \times S$.
    We define $\cdot$ pointwise 
        such that $(a, b) \cdot (c, d) = (a \cdot c, b \cdot d)$.
    Similarly we define $+$ pointwise 
        such that $(a, b) + (c, d) = (a + c, b + d)$.
\end{definition}

\begin{definition}
    Let $R, S$ be arbitrary rings.
    Let $(r_1, r_2, \ldots, r_n, s_1, s_2, \ldots, s_n),
       (r'_1, r'_2, \ldots, r'_n, s'_1, s'_2, \ldots, s'_n)
        \in (R_1 \times R_2 \times \cdots \times R_n) \times (S_1 \times S_2 \times \cdots \times S_n)$.
    We define $\cdot$ pointwise 
        such that $(r_1, r_2, \ldots, r_n, s_1, s_2, \ldots, s_n)
             \cdot (r'_1, r'_2, \ldots, r'_n, s'_1, s'_2, \ldots, s'_n) 
                 = (r_1 \cdot r'_1, r_2 \cdot r'_2, \ldots, r_n \cdot r'_n,
                    s_1 \cdot s'_1, s_2 \cdot s'_2, \ldots, s_n \cdot s'_n)$.
    Similarly we define $+$ pointwise 
        such that $(r_1, r_2, \ldots, r_n, s_1, s_2, \ldots, s_n)
                 + (r'_1, r'_2, \ldots, r'_n, s'_1, s'_2, \ldots, s'_n) 
                 = (r_1 + r'_1, r_2 + r'_2, \ldots, r_n + r'_n,
                    s_1 + s'_1, s_2 + s'_2, \ldots, s_n + s'_n)$.
\end{definition}

\begin{proof}
    Let $R$ and $S$ be arbitrary rings.
    Let $(a, b), (c, d), (e, f) \in R \times S$.

    (\textbf{Rule 1})
    \begin{align*}
        (a, b) + (c, d) &= (a + c, b + d)
                        &= (c + a, d + b)
                        &= (c, d) + (a, b)
    \end{align*}
    (\textbf{Rule 2})
    \begin{align*}
        (a, b) + [(c, d) + (e, f)]
            &= (a, b) + (c + e, d + f) \\
            &= (a + (c + e), b + (d + f)) \\
            &= ((a + c) + e, (b + d) + f) \\
            &= (a + c, b + d) + (e, f) \\
            &= [(a, b) + (c, d)] + (e, f)
    \end{align*}
    (\textbf{Rule 3})
    \begin{align*}
        (a, b) + (0, 0) &= (a + 0, b + 0) \\
                        &= (a, b)
    \end{align*}
    (\textbf{Rule 4})
    \begin{align*}
        (a, b) + (-a, -b) &= (a - a, b - b) \\
                          &= (0, 0)
    \end{align*}
    (\textbf{Rule 5})
    \begin{align*}
        (a, b) [(c, d) (e, f)] &= (a, b)(ce, df) \\
                               &= (a(ce), b(df)) \\
                               &= ((ac)e, (bd)f) \\
                               &= (ac, bd)(e, f) \\
                               &= [(a, b)(c, d)](e, f)
    \end{align*}
    (\textbf{Rule 6 Left})
    \begin{align*}
        (a, b)[(c, d) + (e, f)] &= (a, b)(c + e, d + f) \\
                                &= (a(c + e), b(d + f)) \\
                                &= (ac + ae, bd + bf) \\
                                &= (ac, bd) + (ae, bf) \\
                                &= (a, b)(c, d) + (a, b)(e, f)
    \end{align*}
    (\textbf{Rule 6 Right})
    \begin{align*}
        [(a, b)(c, d)](e, f) &= (a + c, b + d)(e, f) \\
                             &= ((a + c)e, (b + d)f) \\
                             &= (ae + ce, bf + df) \\
                             &= (ae, bf) + (ce, df) \\
                             &= (a, b)(e, f) + (c, d)(e, f)
    \end{align*}
    We proceed via induction to prove the direct product of an arbitrary number of rings
        is indeed a ring.
    The proceeding part of this proof shows the base case.
    Suppose it holds for some $R_1 \times R_2 \times \cdots \times R_n$ where $n \in \mathbb{N}$.
    Consider $R_1 \times R_2 \times \cdots \times R_n \times R_{n + 1}$.
    Now, $R_1 \times R_2 \times \cdots \times R_n$ is a ring and $R_{n + 1}$ is a ring 
    and our proceeding part of the proof works for arbitrary rings thus 
    $R_1 \times R_2 \times \cdots \times R_n \times R_{n + 1}$ is a ring.
\end{proof}

\begin{tcolorbox}[title=Problem 16, breakable]
    Find an example of $M_2(\mathbb{Z})$ to show that 
    $(a + b)^2$ is not necessarily equal to 
    $a^2 + 2ab + b^2$. (Recall that $2ab = ab + ab$.) What is 
    the correct expansion of $(a + b)^2$ for an arbitrary ring?
    What can you say of the ring is commutative.
\end{tcolorbox}

\textbf{Solution:}
\[
    (a + b)^2 =
    \left(\begin{bmatrix}
    0 & 1 \\
    1 & 1
    \end{bmatrix}
    +
    \begin{bmatrix}
    1 & 1 \\
    1 & 0
    \end{bmatrix}\right)^2
    =
    \begin{bmatrix}
    1 & 2 \\
    2 & 1
    \end{bmatrix}^2
    =
    \begin{bmatrix}
    1\cdot1 + 2\cdot2 & 1\cdot2 + 2\cdot1 \\
    2\cdot1 + 1\cdot2 & 2\cdot2 + 1\cdot1
    \end{bmatrix}
    =
    \begin{bmatrix}
    5 & 4 \\
    4 & 5
    \end{bmatrix}.
\]
\[
    a^2 + 2ab + b^2
    =
    \begin{bmatrix}
    0 & 1 \\
    1 & 1
    \end{bmatrix}^2
    + 2
    \begin{bmatrix}
    0 & 1 \\
    1 & 1
    \end{bmatrix}
    \begin{bmatrix}
    1 & 1 \\
    1 & 0
    \end{bmatrix}
    +
    \begin{bmatrix}
    1 & 1 \\
    1 & 0
    \end{bmatrix}^2
    =
    \begin{bmatrix}
    5 & 2 \\
    6 & 5
    \end{bmatrix}.
\]

The correct expansion is 
    $(a + b)^2 = a^2 + ab + ba + b^2$.

If the ring is commutative then $ab = ba$
    thus $(a + b)^2 = a^2 + 2ab + b^2$.

\begin{tcolorbox}[title=Problem 17, breakable]
    (This exercise extends the discussion of Exercise 16.)
    Let $R$ be a commutative ring and $a, b \in R$.
    Then prove the \emph{binomial theorem}
    for $R$, by induction on $n$: Namely, show that 
    \[(a + b)^n = \sum_{k = 0}^{n} \binom{n}{k} a^{n - k}b^k.\]
\end{tcolorbox}

\begin{proof}
    (\textbf{Base Case:}) Trivial.

    (\textbf{Induction Step:}) Assume the formula holds for $n - 1$, thus:
    \begin{align*}
        \sum_{k = 0}^{n - 1} \binom{n - 1}{k} x^k y^{(n-1)-k} = (x+y)^{n - 1}
    \end{align*}
    Then:
    \begin{align*}
        {(x+y)}^n & = {(x+y)}^{n - 1} \cdot (x + y)                                                                                              \\
                  & = \left(\sum_{k = 0}^{n - 1} \binom{n - 1}{k} x^k y^{(n-1)-k}\right) \cdot (x + y)                                              \\
                  & = x \cdot \sum_{k = 0}^{n - 1} \binom{n - 1}{k} x^k y^{(n-1)-k} + y \cdot \sum_{k = 0}^{n - 1} \binom{n - 1}{k} x^k y^{(n-1)-k} \\
                  & = \sum_{k = 1}^{n} \binom{n - 1}{k - 1} x^k y^{n - k} + \sum_{k = 0}^{n - 1} \binom{n - 1}{k} x^k y^{n - k}                     \\
                  & = \sum_{k = 0}^{n} \left( \binom{n - 1}{k - 1} + \binom{n - 1}{k} \right) x^k y^{n - k}                                         \\
                  & = \sum_{k = 0}^{n} \binom{n}{k} x^k y^{n - k}
    \end{align*}
\end{proof}

\begin{tcolorbox}[title=Problem 18, breakable]
    Suppose $a \cdot a = a$ for every element $a$ in a ring $R$.
    (Elements $a$ in a ring where $a^2 = a$ are called \textbf{idempotent}.)

    (a) Show that $a = -a$.

    (b) Now show that $R$ is commutative.
\end{tcolorbox}

\begin{proof}
    Since every element of $R$ is idempotent, we have $a^2 = a$.
    Because $a + a \in R$, it is also idempotent, so $(a + a)^2 = a + a$.
    Expanding using distributivity gives $(a + a)^2 = a^2 + a^2 + a^2 + a^2 = 4a^2$.
    Substituting $a^2 = a$ yields $4a = 2a$.
    Subtracting $2a$ from both sides gives $2a = 0$.
    Thus $a + a = 0$, and therefore $a = -a$.
\end{proof}

\begin{proof}
    Let $a,b \in R$.
    Since every element is idempotent, we have $(a + b)^2 = a + b$.
    Expanding the left-hand side gives $(a + b)^2 = a^2 + ab + ba + b^2$.
    Using $a^2 = a$ and $b^2 = b$, this becomes $a + ab + ba + b = a + b$.
    Canceling $a + b$ from both sides yields $ab + ba = 0$.
    From part (a), every element equals its additive inverse, so $ab = -ab = ba$.
    Hence $ab = ba$, and $R$ is commutative.
\end{proof}

\begin{tcolorbox}[title=Problem 19, breakable]
    Let $S = \{(x_2, x_2, x_3, \ldots) \mid x_i \in \mathbb{R}\}$,
    the real-valued sequences. Define addition and multiplication 
    on $S$ coordinate-wise (see Exercise $13$ and $14$). Show that 
    $S$ is a commutative ring.
\end{tcolorbox}

\begin{proof}
    We know from problem $14$ that $S$ is a ring.
    The $\cdot$ operation for the direct product is based 
        on the multiplication of $\mathbb{R}$ which commutes.
    Thus $S$ is a commutative ring.
\end{proof}

\begin{tcolorbox}[title=Problem 20, breakable]
    Let $X$ be some arbitrary set, an $P(X)$ the 
    set of all subsets of $X$.
    In Example 1.1 we proved that if $X$ has $n$
    elements, then $P(X)$ has $2^n$ elements;
    we are here allowing the possibility that $X$
    (and hence $P(X)$) has \emph{infinitely} many 
    elements. Define operations on 
    $P(X)$ as follows, where $a, b \in P(X)$:
    \[a + b = (a \cup b) \setminus (a \cap b) \text{ and } ab = a \cap b\]
    (Addition here is often called the 
    \textbf{symmetric difference} of the two sets $a, b$.)
    Prove that $P(X)$ is commutative ring.
    ($P(X)$ is called the \textbf{power set} for the set $X$.)
\end{tcolorbox}

\begin{proof}
    Let $a, b, c \in P(X)$.

    (\textbf{Rule 1}) 
    \[
    a + b = (a \cup b) \setminus (a \cap b)
          = (b \cup a) \setminus (b \cap a)
          = b + a.
    \]
    (\textbf{Rule 2})
    \[
    (a + b) + c = a + (b + c).
    \]
    (\textbf{Rule 3})
    \[
    a + \emptyset = (a \cup \emptyset) \setminus (a \cap \emptyset) = a.
    \]
    (\textbf{Rule 4})
    \[
    a + a = (a \cup a) \setminus (a \cap a) = a \setminus a = \emptyset.
    \]
    (\textbf{Rule 5})
    The intersection is associative thus
    \[
    (ab)c = a(bc).
    \]
    (\textbf{Rule 6 Left}) 
    \[
    a(b + c) = a \cap \bigl((b \cup c) \setminus (b \cap c)\bigr)
             = (a \cap b) + (a \cap c)
             = ab + ac.
    \]
    (\textbf{Rule 6 Right}) 
    \[
    (a + b)c = \bigl((a \cup b) \setminus (a \cap b)\bigr) \cap c
             = (a \cap c) + (b \cap c)
             = ac + bc.
    \]
\end{proof}


\begin{tcolorbox}[title=Problem 23, breakable]
    Let $R$ be any commuatative ring.
    Let $R[x]$ be the collection of polynomials with 
    coefficients from $R$. Show that $R[x]$ is a ring.
\end{tcolorbox} 
\begin{proof}
    Since addition and multiplication in $R[x]$ are defined coefficient-wise
    using the operations in the ring $R$, and since $R$ satisfies the ring axioms,
    it follows that $R[x]$ also satisfies the ring axioms. Thus $R[x]$ is a ring.
\end{proof}
